{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           volume        CODi         SSi      NH3-Ni         TPi         TNi  \\\n",
      "count  365.000000  365.000000  365.000000  365.000000  365.000000  365.000000   \n",
      "mean    16.020453  343.801644  142.701370   32.768767    4.211425   44.811781   \n",
      "std      1.112309   94.357110   44.710622    6.057496    0.856886    7.304578   \n",
      "min     10.751200   95.600000   46.000000   11.800000    1.150000   21.200000   \n",
      "25%     15.449400  280.000000  112.000000   29.300000    3.800000   40.800000   \n",
      "50%     16.108700  333.000000  137.000000   32.400000    4.200000   45.600000   \n",
      "75%     16.670600  403.000000  165.000000   36.200000    4.690000   49.200000   \n",
      "max     18.560100  722.000000  401.000000   64.300000    7.370000   75.900000   \n",
      "\n",
      "             CODe         SSe       NH3Ne         TNe           T  \n",
      "count  365.000000  365.000000  365.000000  365.000000  365.000000  \n",
      "mean    16.841644    4.597260    0.395529    8.231014   21.377808  \n",
      "std      4.263552    0.922546    0.282726    1.155829    6.132509  \n",
      "min      9.000000    2.000000    0.075300    4.400000    0.000000  \n",
      "25%     13.000000    4.000000    0.198000    7.600000   17.200000  \n",
      "50%     17.000000    4.000000    0.315000    8.420000   21.700000  \n",
      "75%     20.000000    5.000000    0.512000    9.110000   26.300000  \n",
      "max     31.200000    8.000000    1.980000   10.900000   30.900000  \n",
      "['volume', 'CODi', 'SSi', 'NH3-Ni', 'TPi', 'TNi', 'CODe', 'SSe', 'NH3Ne', 'TNe', 'T']\n",
      "240\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 5, 32)             4224      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 10,497\n",
      "Trainable params: 10,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 192 samples, validate on 48 samples\n",
      "Epoch 1/400\n",
      "192/192 [==============================] - 2s 11ms/step - loss: 0.7826 - val_loss: 0.1325\n",
      "Epoch 2/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.2941 - val_loss: 0.0575\n",
      "Epoch 3/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1405 - val_loss: 0.3218\n",
      "Epoch 4/400\n",
      "192/192 [==============================] - 0s 255us/step - loss: 0.2933 - val_loss: 0.2771\n",
      "Epoch 5/400\n",
      "192/192 [==============================] - 0s 239us/step - loss: 0.2546 - val_loss: 0.2512\n",
      "Epoch 6/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.2387 - val_loss: 0.2310\n",
      "Epoch 7/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.2178 - val_loss: 0.2124\n",
      "Epoch 8/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.2041 - val_loss: 0.2055\n",
      "Epoch 9/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1967 - val_loss: 0.2017\n",
      "Epoch 10/400\n",
      "192/192 [==============================] - 0s 317us/step - loss: 0.1892 - val_loss: 0.1908\n",
      "Epoch 11/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1882 - val_loss: 0.1866\n",
      "Epoch 12/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1682 - val_loss: 0.1784\n",
      "Epoch 13/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1653 - val_loss: 0.1705\n",
      "Epoch 14/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1679 - val_loss: 0.1703\n",
      "Epoch 15/400\n",
      "192/192 [==============================] - 0s 447us/step - loss: 0.1682 - val_loss: 0.1692\n",
      "Epoch 16/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1645 - val_loss: 0.1646\n",
      "Epoch 17/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1638 - val_loss: 0.1558\n",
      "Epoch 18/400\n",
      "192/192 [==============================] - 0s 312us/step - loss: 0.1550 - val_loss: 0.1548\n",
      "Epoch 19/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.1484 - val_loss: 0.1551\n",
      "Epoch 20/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1522 - val_loss: 0.1528\n",
      "Epoch 21/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1521 - val_loss: 0.1519\n",
      "Epoch 22/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1566 - val_loss: 0.1492\n",
      "Epoch 23/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1453 - val_loss: 0.1524\n",
      "Epoch 24/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1448 - val_loss: 0.1497\n",
      "Epoch 25/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1473 - val_loss: 0.1471\n",
      "Epoch 26/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1403 - val_loss: 0.1369\n",
      "Epoch 27/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1423 - val_loss: 0.1402\n",
      "Epoch 28/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1347 - val_loss: 0.1416\n",
      "Epoch 29/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1362 - val_loss: 0.1383\n",
      "Epoch 30/400\n",
      "192/192 [==============================] - 0s 322us/step - loss: 0.1319 - val_loss: 0.1270\n",
      "Epoch 31/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1337 - val_loss: 0.1198\n",
      "Epoch 32/400\n",
      "192/192 [==============================] - 0s 338us/step - loss: 0.1298 - val_loss: 0.1211\n",
      "Epoch 33/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.1325 - val_loss: 0.1307\n",
      "Epoch 34/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1375 - val_loss: 0.1287\n",
      "Epoch 35/400\n",
      "192/192 [==============================] - 0s 431us/step - loss: 0.1319 - val_loss: 0.1287\n",
      "Epoch 36/400\n",
      "192/192 [==============================] - 0s 317us/step - loss: 0.1333 - val_loss: 0.1309\n",
      "Epoch 37/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.1313 - val_loss: 0.1356\n",
      "Epoch 38/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1293 - val_loss: 0.1417\n",
      "Epoch 39/400\n",
      "192/192 [==============================] - 0s 374us/step - loss: 0.1321 - val_loss: 0.1438\n",
      "Epoch 40/400\n",
      "192/192 [==============================] - 0s 327us/step - loss: 0.1317 - val_loss: 0.1458\n",
      "Epoch 41/400\n",
      "192/192 [==============================] - 0s 275us/step - loss: 0.1335 - val_loss: 0.1455\n",
      "Epoch 42/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1274 - val_loss: 0.1424\n",
      "Epoch 43/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1395 - val_loss: 0.1447\n",
      "Epoch 44/400\n",
      "192/192 [==============================] - 0s 306us/step - loss: 0.1314 - val_loss: 0.1414\n",
      "Epoch 45/400\n",
      "192/192 [==============================] - 0s 296us/step - loss: 0.1302 - val_loss: 0.1425\n",
      "Epoch 46/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1364 - val_loss: 0.1433\n",
      "Epoch 47/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.1365 - val_loss: 0.1387\n",
      "Epoch 48/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1272 - val_loss: 0.1465\n",
      "Epoch 49/400\n",
      "192/192 [==============================] - 0s 270us/step - loss: 0.1341 - val_loss: 0.1440\n",
      "Epoch 50/400\n",
      "192/192 [==============================] - 0s 270us/step - loss: 0.1395 - val_loss: 0.1434\n",
      "Epoch 51/400\n",
      "192/192 [==============================] - 0s 317us/step - loss: 0.1285 - val_loss: 0.1407\n",
      "Epoch 52/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1329 - val_loss: 0.1472\n",
      "Epoch 53/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1282 - val_loss: 0.1415\n",
      "Epoch 54/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1262 - val_loss: 0.1448\n",
      "Epoch 55/400\n",
      "192/192 [==============================] - 0s 275us/step - loss: 0.1304 - val_loss: 0.1454\n",
      "Epoch 56/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1306 - val_loss: 0.1464\n",
      "Epoch 57/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1340 - val_loss: 0.1441\n",
      "Epoch 58/400\n",
      "192/192 [==============================] - 0s 338us/step - loss: 0.1266 - val_loss: 0.1356\n",
      "Epoch 59/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 338us/step - loss: 0.1227 - val_loss: 0.1335\n",
      "Epoch 60/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1263 - val_loss: 0.1409\n",
      "Epoch 61/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1310 - val_loss: 0.1444\n",
      "Epoch 62/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1303 - val_loss: 0.1436\n",
      "Epoch 63/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1246 - val_loss: 0.1433\n",
      "Epoch 64/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1271 - val_loss: 0.1353\n",
      "Epoch 65/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1213 - val_loss: 0.1385\n",
      "Epoch 66/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1198 - val_loss: 0.1385\n",
      "Epoch 67/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1236 - val_loss: 0.1411\n",
      "Epoch 68/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1223 - val_loss: 0.1421\n",
      "Epoch 69/400\n",
      "192/192 [==============================] - 0s 395us/step - loss: 0.1271 - val_loss: 0.1416\n",
      "Epoch 70/400\n",
      "192/192 [==============================] - 0s 561us/step - loss: 0.1247 - val_loss: 0.1383\n",
      "Epoch 71/400\n",
      "192/192 [==============================] - 0s 265us/step - loss: 0.1200 - val_loss: 0.1401\n",
      "Epoch 72/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1250 - val_loss: 0.1376\n",
      "Epoch 73/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1230 - val_loss: 0.1381\n",
      "Epoch 74/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1237 - val_loss: 0.1356\n",
      "Epoch 75/400\n",
      "192/192 [==============================] - 0s 312us/step - loss: 0.1178 - val_loss: 0.1246\n",
      "Epoch 76/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.1164 - val_loss: 0.1260\n",
      "Epoch 77/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1120 - val_loss: 0.1286\n",
      "Epoch 78/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1139 - val_loss: 0.1230\n",
      "Epoch 79/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1166 - val_loss: 0.1279\n",
      "Epoch 80/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1182 - val_loss: 0.1283\n",
      "Epoch 81/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1161 - val_loss: 0.1225\n",
      "Epoch 82/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1123 - val_loss: 0.1272\n",
      "Epoch 83/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1156 - val_loss: 0.1298\n",
      "Epoch 84/400\n",
      "192/192 [==============================] - 0s 265us/step - loss: 0.1249 - val_loss: 0.1304\n",
      "Epoch 85/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1209 - val_loss: 0.1302\n",
      "Epoch 86/400\n",
      "192/192 [==============================] - 0s 255us/step - loss: 0.1173 - val_loss: 0.1249\n",
      "Epoch 87/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1156 - val_loss: 0.1265\n",
      "Epoch 88/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1173 - val_loss: 0.1217\n",
      "Epoch 89/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1134 - val_loss: 0.1235\n",
      "Epoch 90/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1147 - val_loss: 0.1189\n",
      "Epoch 91/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1126 - val_loss: 0.1229\n",
      "Epoch 92/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1155 - val_loss: 0.1248\n",
      "Epoch 93/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.1110 - val_loss: 0.1262\n",
      "Epoch 94/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1098 - val_loss: 0.1219\n",
      "Epoch 95/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1102 - val_loss: 0.1232\n",
      "Epoch 96/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1105 - val_loss: 0.1272\n",
      "Epoch 97/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1121 - val_loss: 0.1249\n",
      "Epoch 98/400\n",
      "192/192 [==============================] - 0s 280us/step - loss: 0.1116 - val_loss: 0.1230\n",
      "Epoch 99/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1191 - val_loss: 0.1259\n",
      "Epoch 100/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1150 - val_loss: 0.1292\n",
      "Epoch 101/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1138 - val_loss: 0.1332\n",
      "Epoch 102/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1133 - val_loss: 0.1303\n",
      "Epoch 103/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1150 - val_loss: 0.1360\n",
      "Epoch 104/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1137 - val_loss: 0.1238\n",
      "Epoch 105/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1085 - val_loss: 0.1220\n",
      "Epoch 106/400\n",
      "192/192 [==============================] - 0s 270us/step - loss: 0.1094 - val_loss: 0.1274\n",
      "Epoch 107/400\n",
      "192/192 [==============================] - 0s 255us/step - loss: 0.1151 - val_loss: 0.1264\n",
      "Epoch 108/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1136 - val_loss: 0.1191\n",
      "Epoch 109/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1166 - val_loss: 0.1225\n",
      "Epoch 110/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1139 - val_loss: 0.1239\n",
      "Epoch 111/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1123 - val_loss: 0.1237\n",
      "Epoch 112/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.1082 - val_loss: 0.1199\n",
      "Epoch 113/400\n",
      "192/192 [==============================] - 0s 748us/step - loss: 0.1105 - val_loss: 0.1217\n",
      "Epoch 114/400\n",
      "192/192 [==============================] - 0s 421us/step - loss: 0.1138 - val_loss: 0.1217\n",
      "Epoch 115/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1109 - val_loss: 0.1203\n",
      "Epoch 116/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1078 - val_loss: 0.1146\n",
      "Epoch 117/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1112 - val_loss: 0.1182\n",
      "Epoch 118/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1048 - val_loss: 0.1230\n",
      "Epoch 119/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1083 - val_loss: 0.1199\n",
      "Epoch 120/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1114 - val_loss: 0.1116\n",
      "Epoch 121/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.1118 - val_loss: 0.1125\n",
      "Epoch 122/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1122 - val_loss: 0.1116\n",
      "Epoch 123/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1086 - val_loss: 0.1115\n",
      "Epoch 124/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1045 - val_loss: 0.1109\n",
      "Epoch 125/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.1039 - val_loss: 0.1054\n",
      "Epoch 126/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1046 - val_loss: 0.1126\n",
      "Epoch 127/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1039 - val_loss: 0.1079\n",
      "Epoch 128/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1076 - val_loss: 0.1086\n",
      "Epoch 129/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1062 - val_loss: 0.1087\n",
      "Epoch 130/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1082 - val_loss: 0.1032\n",
      "Epoch 131/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1023 - val_loss: 0.1029\n",
      "Epoch 132/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1068 - val_loss: 0.1019\n",
      "Epoch 133/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0961 - val_loss: 0.1111\n",
      "Epoch 134/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1036 - val_loss: 0.1106\n",
      "Epoch 135/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1084 - val_loss: 0.1089\n",
      "Epoch 136/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0988 - val_loss: 0.1124\n",
      "Epoch 137/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 234us/step - loss: 0.1040 - val_loss: 0.1095\n",
      "Epoch 138/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1023 - val_loss: 0.1137\n",
      "Epoch 139/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.1064 - val_loss: 0.1141\n",
      "Epoch 140/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1062 - val_loss: 0.1101\n",
      "Epoch 141/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1035 - val_loss: 0.1102\n",
      "Epoch 142/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1055 - val_loss: 0.1101\n",
      "Epoch 143/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1000 - val_loss: 0.1098\n",
      "Epoch 144/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1019 - val_loss: 0.1115\n",
      "Epoch 145/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1012 - val_loss: 0.1113\n",
      "Epoch 146/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1027 - val_loss: 0.1131\n",
      "Epoch 147/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1050 - val_loss: 0.1159\n",
      "Epoch 148/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.1034 - val_loss: 0.1110\n",
      "Epoch 149/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1034 - val_loss: 0.1127\n",
      "Epoch 150/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1058 - val_loss: 0.1143\n",
      "Epoch 151/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1036 - val_loss: 0.1088\n",
      "Epoch 152/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.0994 - val_loss: 0.1108\n",
      "Epoch 153/400\n",
      "192/192 [==============================] - 0s 312us/step - loss: 0.0993 - val_loss: 0.1127\n",
      "Epoch 154/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1034 - val_loss: 0.1122\n",
      "Epoch 155/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1043 - val_loss: 0.1122\n",
      "Epoch 156/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1040 - val_loss: 0.1112\n",
      "Epoch 157/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0973 - val_loss: 0.1106\n",
      "Epoch 158/400\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.115 - 0s 166us/step - loss: 0.0991 - val_loss: 0.1145\n",
      "Epoch 159/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1040 - val_loss: 0.1153\n",
      "Epoch 160/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1028 - val_loss: 0.1096\n",
      "Epoch 161/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.1014 - val_loss: 0.1100\n",
      "Epoch 162/400\n",
      "192/192 [==============================] - 0s 281us/step - loss: 0.0983 - val_loss: 0.1117\n",
      "Epoch 163/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1041 - val_loss: 0.1101\n",
      "Epoch 164/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1018 - val_loss: 0.1094\n",
      "Epoch 165/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1059 - val_loss: 0.1128\n",
      "Epoch 166/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1009 - val_loss: 0.1129\n",
      "Epoch 167/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1028 - val_loss: 0.1132\n",
      "Epoch 168/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.1004 - val_loss: 0.1143\n",
      "Epoch 169/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1008 - val_loss: 0.1148\n",
      "Epoch 170/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1045 - val_loss: 0.1163\n",
      "Epoch 171/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1065 - val_loss: 0.1188\n",
      "Epoch 172/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1085 - val_loss: 0.1194\n",
      "Epoch 173/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1078 - val_loss: 0.1168\n",
      "Epoch 174/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1101 - val_loss: 0.1167\n",
      "Epoch 175/400\n",
      "192/192 [==============================] - 0s 150us/step - loss: 0.1074 - val_loss: 0.1104\n",
      "Epoch 176/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.1054 - val_loss: 0.1139\n",
      "Epoch 177/400\n",
      "192/192 [==============================] - 0s 239us/step - loss: 0.1089 - val_loss: 0.1137\n",
      "Epoch 178/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1070 - val_loss: 0.1092\n",
      "Epoch 179/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1028 - val_loss: 0.1085\n",
      "Epoch 180/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1049 - val_loss: 0.1089\n",
      "Epoch 181/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1048 - val_loss: 0.1109\n",
      "Epoch 182/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1056 - val_loss: 0.1127\n",
      "Epoch 183/400\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.120 - 0s 270us/step - loss: 0.1042 - val_loss: 0.1136\n",
      "Epoch 184/400\n",
      "192/192 [==============================] - 0s 317us/step - loss: 0.1062 - val_loss: 0.1148\n",
      "Epoch 185/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1028 - val_loss: 0.1122\n",
      "Epoch 186/400\n",
      "192/192 [==============================] - 0s 208us/step - loss: 0.1046 - val_loss: 0.1111\n",
      "Epoch 187/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1050 - val_loss: 0.1094\n",
      "Epoch 188/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1035 - val_loss: 0.1082\n",
      "Epoch 189/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.0975 - val_loss: 0.1155\n",
      "Epoch 190/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.1053 - val_loss: 0.1154\n",
      "Epoch 191/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.1058 - val_loss: 0.1146\n",
      "Epoch 192/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1080 - val_loss: 0.1132\n",
      "Epoch 193/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.1076 - val_loss: 0.1086\n",
      "Epoch 194/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1046 - val_loss: 0.1121\n",
      "Epoch 195/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1085 - val_loss: 0.1112\n",
      "Epoch 196/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1049 - val_loss: 0.1129\n",
      "Epoch 197/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1099 - val_loss: 0.1137\n",
      "Epoch 198/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.1046 - val_loss: 0.1144\n",
      "Epoch 199/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1071 - val_loss: 0.1142\n",
      "Epoch 200/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1004 - val_loss: 0.1081\n",
      "Epoch 201/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1031 - val_loss: 0.1082\n",
      "Epoch 202/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1034 - val_loss: 0.1025\n",
      "Epoch 203/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0999 - val_loss: 0.1072\n",
      "Epoch 204/400\n",
      "192/192 [==============================] - 0s 286us/step - loss: 0.1020 - val_loss: 0.1031\n",
      "Epoch 205/400\n",
      "192/192 [==============================] - 0s 270us/step - loss: 0.1012 - val_loss: 0.1094\n",
      "Epoch 206/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1078 - val_loss: 0.1084\n",
      "Epoch 207/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1001 - val_loss: 0.1071\n",
      "Epoch 208/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.0972 - val_loss: 0.1098\n",
      "Epoch 209/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1004 - val_loss: 0.1043\n",
      "Epoch 210/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0974 - val_loss: 0.1112\n",
      "Epoch 211/400\n",
      "192/192 [==============================] - 0s 286us/step - loss: 0.1049 - val_loss: 0.1122\n",
      "Epoch 212/400\n",
      "192/192 [==============================] - 0s 301us/step - loss: 0.1088 - val_loss: 0.1081\n",
      "Epoch 213/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1013 - val_loss: 0.1053\n",
      "Epoch 214/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 234us/step - loss: 0.1008 - val_loss: 0.1115\n",
      "Epoch 215/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1091 - val_loss: 0.1120\n",
      "Epoch 216/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1055 - val_loss: 0.1126\n",
      "Epoch 217/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1060 - val_loss: 0.1129\n",
      "Epoch 218/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1027 - val_loss: 0.1143\n",
      "Epoch 219/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1051 - val_loss: 0.1146\n",
      "Epoch 220/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1053 - val_loss: 0.1134\n",
      "Epoch 221/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1013 - val_loss: 0.1087\n",
      "Epoch 222/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1028 - val_loss: 0.1073\n",
      "Epoch 223/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1018 - val_loss: 0.1102\n",
      "Epoch 224/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1022 - val_loss: 0.1113\n",
      "Epoch 225/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.1029 - val_loss: 0.1144\n",
      "Epoch 226/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1004 - val_loss: 0.1097\n",
      "Epoch 227/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.1006 - val_loss: 0.1059\n",
      "Epoch 228/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1012 - val_loss: 0.1105\n",
      "Epoch 229/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1048 - val_loss: 0.1136\n",
      "Epoch 230/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1049 - val_loss: 0.1129\n",
      "Epoch 231/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1033 - val_loss: 0.1136\n",
      "Epoch 232/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1002 - val_loss: 0.1134\n",
      "Epoch 233/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0996 - val_loss: 0.1120\n",
      "Epoch 234/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1033 - val_loss: 0.1084\n",
      "Epoch 235/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.0931 - val_loss: 0.1117\n",
      "Epoch 236/400\n",
      "192/192 [==============================] - 0s 140us/step - loss: 0.1057 - val_loss: 0.1151\n",
      "Epoch 237/400\n",
      "192/192 [==============================] - 0s 286us/step - loss: 0.1064 - val_loss: 0.1098\n",
      "Epoch 238/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.0992 - val_loss: 0.1071\n",
      "Epoch 239/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1025 - val_loss: 0.1049\n",
      "Epoch 240/400\n",
      "192/192 [==============================] - 0s 296us/step - loss: 0.0998 - val_loss: 0.1063\n",
      "Epoch 241/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1020 - val_loss: 0.1089\n",
      "Epoch 242/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1009 - val_loss: 0.1128\n",
      "Epoch 243/400\n",
      "192/192 [==============================] - 0s 296us/step - loss: 0.0990 - val_loss: 0.1169\n",
      "Epoch 244/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.1039 - val_loss: 0.1060\n",
      "Epoch 245/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.0997 - val_loss: 0.1063\n",
      "Epoch 246/400\n",
      "192/192 [==============================] - 0s 275us/step - loss: 0.1041 - val_loss: 0.1105\n",
      "Epoch 247/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.1002 - val_loss: 0.1087\n",
      "Epoch 248/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1062 - val_loss: 0.1075\n",
      "Epoch 249/400\n",
      "192/192 [==============================] - 0s 140us/step - loss: 0.1020 - val_loss: 0.1048\n",
      "Epoch 250/400\n",
      "192/192 [==============================] - 0s 254us/step - loss: 0.1013 - val_loss: 0.1037\n",
      "Epoch 251/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.0981 - val_loss: 0.1056\n",
      "Epoch 252/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.0967 - val_loss: 0.1101\n",
      "Epoch 253/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.0994 - val_loss: 0.1140\n",
      "Epoch 254/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.0990 - val_loss: 0.1124\n",
      "Epoch 255/400\n",
      "192/192 [==============================] - 0s 322us/step - loss: 0.1000 - val_loss: 0.1123\n",
      "Epoch 256/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1044 - val_loss: 0.1143\n",
      "Epoch 257/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0996 - val_loss: 0.1083\n",
      "Epoch 258/400\n",
      "192/192 [==============================] - 0s 255us/step - loss: 0.1004 - val_loss: 0.1115\n",
      "Epoch 259/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.1032 - val_loss: 0.1061\n",
      "Epoch 260/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.0961 - val_loss: 0.1117\n",
      "Epoch 261/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1032 - val_loss: 0.1058\n",
      "Epoch 262/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0965 - val_loss: 0.1118\n",
      "Epoch 263/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1060 - val_loss: 0.1109\n",
      "Epoch 264/400\n",
      "192/192 [==============================] - 0s 249us/step - loss: 0.0998 - val_loss: 0.1088\n",
      "Epoch 265/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0987 - val_loss: 0.1088\n",
      "Epoch 266/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.1038 - val_loss: 0.1111\n",
      "Epoch 267/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.0991 - val_loss: 0.1132\n",
      "Epoch 268/400\n",
      "192/192 [==============================] - 0s 239us/step - loss: 0.1031 - val_loss: 0.1096\n",
      "Epoch 269/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.0987 - val_loss: 0.1102\n",
      "Epoch 270/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1028 - val_loss: 0.1132\n",
      "Epoch 271/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1001 - val_loss: 0.1127\n",
      "Epoch 272/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0999 - val_loss: 0.1123\n",
      "Epoch 273/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0994 - val_loss: 0.1078\n",
      "Epoch 274/400\n",
      "192/192 [==============================] - 0s 281us/step - loss: 0.1019 - val_loss: 0.1082\n",
      "Epoch 275/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.0998 - val_loss: 0.1068\n",
      "Epoch 276/400\n",
      "192/192 [==============================] - 0s 218us/step - loss: 0.1016 - val_loss: 0.1124\n",
      "Epoch 277/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.0986 - val_loss: 0.1140\n",
      "Epoch 278/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1030 - val_loss: 0.1129\n",
      "Epoch 279/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.0999 - val_loss: 0.1065\n",
      "Epoch 280/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0938 - val_loss: 0.1094\n",
      "Epoch 281/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.1025 - val_loss: 0.1087\n",
      "Epoch 282/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.0963 - val_loss: 0.1120\n",
      "Epoch 283/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1072 - val_loss: 0.1113\n",
      "Epoch 284/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.0997 - val_loss: 0.1112\n",
      "Epoch 285/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0966 - val_loss: 0.1081\n",
      "Epoch 286/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.1013 - val_loss: 0.1063\n",
      "Epoch 287/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0968 - val_loss: 0.1056\n",
      "Epoch 288/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.0983 - val_loss: 0.1003\n",
      "Epoch 289/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0930 - val_loss: 0.1031\n",
      "Epoch 290/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0966 - val_loss: 0.1056\n",
      "Epoch 291/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0948 - val_loss: 0.0982\n",
      "Epoch 292/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 223us/step - loss: 0.0935 - val_loss: 0.1035\n",
      "Epoch 293/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.1027 - val_loss: 0.1020\n",
      "Epoch 294/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.0997 - val_loss: 0.1014\n",
      "Epoch 295/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0964 - val_loss: 0.0985\n",
      "Epoch 296/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0960 - val_loss: 0.1019\n",
      "Epoch 297/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0934 - val_loss: 0.1059\n",
      "Epoch 298/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.0949 - val_loss: 0.1118\n",
      "Epoch 299/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1026 - val_loss: 0.1141\n",
      "Epoch 300/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1079 - val_loss: 0.1137\n",
      "Epoch 301/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0999 - val_loss: 0.1110\n",
      "Epoch 302/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.0983 - val_loss: 0.1029\n",
      "Epoch 303/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.0955 - val_loss: 0.1043\n",
      "Epoch 304/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0989 - val_loss: 0.1068\n",
      "Epoch 305/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.0971 - val_loss: 0.1080\n",
      "Epoch 306/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.0946 - val_loss: 0.1010\n",
      "Epoch 307/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.0968 - val_loss: 0.1032\n",
      "Epoch 308/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.0921 - val_loss: 0.1060\n",
      "Epoch 309/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0962 - val_loss: 0.1051\n",
      "Epoch 310/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0966 - val_loss: 0.1031\n",
      "Epoch 311/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0931 - val_loss: 0.1028\n",
      "Epoch 312/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0930 - val_loss: 0.1043\n",
      "Epoch 313/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.0926 - val_loss: 0.1076\n",
      "Epoch 314/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0982 - val_loss: 0.1069\n",
      "Epoch 315/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0976 - val_loss: 0.1065\n",
      "Epoch 316/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0960 - val_loss: 0.1100\n",
      "Epoch 317/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.0971 - val_loss: 0.1072\n",
      "Epoch 318/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.1005 - val_loss: 0.1075\n",
      "Epoch 319/400\n",
      "192/192 [==============================] - 0s 145us/step - loss: 0.0990 - val_loss: 0.1082\n",
      "Epoch 320/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0987 - val_loss: 0.1106\n",
      "Epoch 321/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.1008 - val_loss: 0.1103\n",
      "Epoch 322/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1006 - val_loss: 0.1118\n",
      "Epoch 323/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1041 - val_loss: 0.1125\n",
      "Epoch 324/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1014 - val_loss: 0.1121\n",
      "Epoch 325/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0945 - val_loss: 0.1069\n",
      "Epoch 326/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0930 - val_loss: 0.1013\n",
      "Epoch 327/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0963 - val_loss: 0.1034\n",
      "Epoch 328/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0941 - val_loss: 0.1077\n",
      "Epoch 329/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0994 - val_loss: 0.0984\n",
      "Epoch 330/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.0946 - val_loss: 0.1005\n",
      "Epoch 331/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0945 - val_loss: 0.1035\n",
      "Epoch 332/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.0979 - val_loss: 0.1050\n",
      "Epoch 333/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0955 - val_loss: 0.1025\n",
      "Epoch 334/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0913 - val_loss: 0.1031\n",
      "Epoch 335/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.0960 - val_loss: 0.1008\n",
      "Epoch 336/400\n",
      "192/192 [==============================] - 0s 857us/step - loss: 0.0941 - val_loss: 0.1064\n",
      "Epoch 337/400\n",
      "192/192 [==============================] - 0s 239us/step - loss: 0.0989 - val_loss: 0.1040\n",
      "Epoch 338/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1023 - val_loss: 0.1042\n",
      "Epoch 339/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0968 - val_loss: 0.1063\n",
      "Epoch 340/400\n",
      "192/192 [==============================] - 0s 156us/step - loss: 0.0961 - val_loss: 0.1018\n",
      "Epoch 341/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0950 - val_loss: 0.1044\n",
      "Epoch 342/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0960 - val_loss: 0.1019\n",
      "Epoch 343/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0950 - val_loss: 0.1001\n",
      "Epoch 344/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0968 - val_loss: 0.1014\n",
      "Epoch 345/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0954 - val_loss: 0.1037\n",
      "Epoch 346/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0989 - val_loss: 0.1021\n",
      "Epoch 347/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1000 - val_loss: 0.1026\n",
      "Epoch 348/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0924 - val_loss: 0.1058\n",
      "Epoch 349/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.0965 - val_loss: 0.1082\n",
      "Epoch 350/400\n",
      "192/192 [==============================] - 0s 265us/step - loss: 0.0958 - val_loss: 0.0986\n",
      "Epoch 351/400\n",
      "192/192 [==============================] - 0s 239us/step - loss: 0.0928 - val_loss: 0.1032\n",
      "Epoch 352/400\n",
      "192/192 [==============================] - 0s 426us/step - loss: 0.0986 - val_loss: 0.0991\n",
      "Epoch 353/400\n",
      "192/192 [==============================] - 0s 275us/step - loss: 0.0945 - val_loss: 0.1032\n",
      "Epoch 354/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.0957 - val_loss: 0.1056\n",
      "Epoch 355/400\n",
      "192/192 [==============================] - 0s 151us/step - loss: 0.0981 - val_loss: 0.1007\n",
      "Epoch 356/400\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.0922 - val_loss: 0.1050\n",
      "Epoch 357/400\n",
      "192/192 [==============================] - 0s 280us/step - loss: 0.0940 - val_loss: 0.1042\n",
      "Epoch 358/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0895 - val_loss: 0.1025\n",
      "Epoch 359/400\n",
      "192/192 [==============================] - 0s 260us/step - loss: 0.0949 - val_loss: 0.0972\n",
      "Epoch 360/400\n",
      "192/192 [==============================] - 0s 234us/step - loss: 0.0893 - val_loss: 0.0965\n",
      "Epoch 361/400\n",
      "192/192 [==============================] - 0s 172us/step - loss: 0.1020 - val_loss: 0.0991\n",
      "Epoch 362/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0922 - val_loss: 0.1019\n",
      "Epoch 363/400\n",
      "192/192 [==============================] - 0s 306us/step - loss: 0.0968 - val_loss: 0.0994\n",
      "Epoch 364/400\n",
      "192/192 [==============================] - 0s 291us/step - loss: 0.0973 - val_loss: 0.1043\n",
      "Epoch 365/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0959 - val_loss: 0.1063\n",
      "Epoch 366/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.0979 - val_loss: 0.1058\n",
      "Epoch 367/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0942 - val_loss: 0.1057\n",
      "Epoch 368/400\n",
      "192/192 [==============================] - 0s 161us/step - loss: 0.0974 - val_loss: 0.0989\n",
      "Epoch 369/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0950 - val_loss: 0.1033\n",
      "Epoch 370/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 187us/step - loss: 0.0967 - val_loss: 0.1029\n",
      "Epoch 371/400\n",
      "192/192 [==============================] - 0s 265us/step - loss: 0.0956 - val_loss: 0.1067\n",
      "Epoch 372/400\n",
      "192/192 [==============================] - 0s 327us/step - loss: 0.0966 - val_loss: 0.1055\n",
      "Epoch 373/400\n",
      "192/192 [==============================] - 0s 535us/step - loss: 0.0954 - val_loss: 0.0965\n",
      "Epoch 374/400\n",
      "192/192 [==============================] - 0s 306us/step - loss: 0.0923 - val_loss: 0.1012\n",
      "Epoch 375/400\n",
      "192/192 [==============================] - 0s 213us/step - loss: 0.0969 - val_loss: 0.0975\n",
      "Epoch 376/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0913 - val_loss: 0.1018\n",
      "Epoch 377/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0899 - val_loss: 0.1032\n",
      "Epoch 378/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.1013 - val_loss: 0.0975\n",
      "Epoch 379/400\n",
      "192/192 [==============================] - 0s 166us/step - loss: 0.0898 - val_loss: 0.0969\n",
      "Epoch 380/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0957 - val_loss: 0.0997\n",
      "Epoch 381/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0936 - val_loss: 0.1026\n",
      "Epoch 382/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0986 - val_loss: 0.1024\n",
      "Epoch 383/400\n",
      "192/192 [==============================] - 0s 197us/step - loss: 0.0947 - val_loss: 0.1004\n",
      "Epoch 384/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1002 - val_loss: 0.1003\n",
      "Epoch 385/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0940 - val_loss: 0.0950\n",
      "Epoch 386/400\n",
      "192/192 [==============================] - 0s 229us/step - loss: 0.1009 - val_loss: 0.0979\n",
      "Epoch 387/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.0972 - val_loss: 0.1019\n",
      "Epoch 388/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.1000 - val_loss: 0.1023\n",
      "Epoch 389/400\n",
      "192/192 [==============================] - 0s 223us/step - loss: 0.1007 - val_loss: 0.1055\n",
      "Epoch 390/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0927 - val_loss: 0.1053\n",
      "Epoch 391/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.0940 - val_loss: 0.1058\n",
      "Epoch 392/400\n",
      "192/192 [==============================] - 0s 171us/step - loss: 0.0974 - val_loss: 0.1062\n",
      "Epoch 393/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.1008 - val_loss: 0.1055\n",
      "Epoch 394/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.0968 - val_loss: 0.1007\n",
      "Epoch 395/400\n",
      "192/192 [==============================] - 0s 177us/step - loss: 0.1004 - val_loss: 0.1045\n",
      "Epoch 396/400\n",
      "192/192 [==============================] - 0s 182us/step - loss: 0.1034 - val_loss: 0.0994\n",
      "Epoch 397/400\n",
      "192/192 [==============================] - 0s 176us/step - loss: 0.0954 - val_loss: 0.1007\n",
      "Epoch 398/400\n",
      "192/192 [==============================] - 0s 187us/step - loss: 0.0948 - val_loss: 0.1002\n",
      "Epoch 399/400\n",
      "192/192 [==============================] - 0s 192us/step - loss: 0.0975 - val_loss: 0.1027\n",
      "Epoch 400/400\n",
      "192/192 [==============================] - 0s 203us/step - loss: 0.0959 - val_loss: 0.1055\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 5, 32)             4224      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 10,497\n",
      "Trainable params: 10,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23801397 0.23440674 0.22876008 0.23940653 0.24804944 0.19238111\n",
      " 0.18682058 0.23182715 0.24658155 0.24731648 0.24646479 0.24620786\n",
      " 0.23923197 0.24165726 0.23368672 0.22696038 0.24058345 0.24245542\n",
      " 0.24522331 0.23436065 0.2381496  0.24001542 0.24875396 0.24854538\n",
      " 0.2485897  0.24754286 0.24724537 0.24170223 0.2406576  0.2437951\n",
      " 0.24804252 0.24130166 0.24380004 0.24392694 0.24393746 0.23457162\n",
      " 0.23726663 0.23668914 0.24047711 0.2475672  0.2478331  0.24796122\n",
      " 0.24834925 0.24762154 0.24977988 0.2450574  0.24012938 0.26343936\n",
      " 0.24792784 0.24929833 0.24521214 0.24835372 0.2467134  0.24237365\n",
      " 0.2402578  0.24020845 0.24148002 0.24626625 0.24502811 0.23493816]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前网络结构预测的TPe出水MRE:0.9033307423710257\n",
      "训练集的真值平均值为：0.139525\n"
     ]
    }
   ],
   "source": [
    "# - coding:utf-8 -\n",
    "'''\n",
    "\n",
    "@author:BlazerLean\n",
    "@time:2020/4/3022:35\n",
    "TPe预测\n",
    "'''\n",
    "\n",
    "pre_column = 'TPe'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 设置打印最大行\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "\n",
    "# 导入数据\n",
    "dataset_path = \"./2019data.xlsx\"\n",
    "column_names = ['data', 'volume', 'CODi', 'BODi', 'SSi', 'NH3-Ni', \n",
    "                'TPi', 'TNi', 'CODe', 'BODe', 'SSe', 'NH3Ne', 'TPe',\n",
    "                'TNe', 'T', 'rain']\n",
    "rawdata = pd.read_excel(dataset_path, names=column_names)\n",
    "\n",
    "\n",
    "# 将降雨中的空值转化为0\n",
    "rawdata = rawdata.replace(np.NaN, 0)\n",
    "# data = data.fillna(0)\n",
    "\n",
    "\n",
    "# 删除相关性较差列数据及时间列\n",
    "del rawdata['data']\n",
    "del rawdata['BODi']\n",
    "del rawdata['BODe']\n",
    "del rawdata['rain']\n",
    "\n",
    "\n",
    "# 将总数据提出来\n",
    "result = rawdata.pop(pre_column)\n",
    "\n",
    "\n",
    "# 输入指标归一化\n",
    "rawdata_stats = rawdata.describe()\n",
    "print(rawdata_stats)\n",
    "train_stats = rawdata_stats.transpose()\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean'])/train_stats['std']\n",
    "\n",
    "\n",
    "rawdata_norm = norm(rawdata)\n",
    "\n",
    "\n",
    "# 整理成RNN输入数据形式\n",
    "datanum = 300  # 使用数据组数////////////////////////////////////////////////////////////////////////////////////////\n",
    "lookback = 5  # 设置输入变量涵盖天数///////////////////////////////////////////////////////////////////////////////\n",
    "paranum = 11  # 设置输入变量的指标个数//////////////////////////////////////////////////////////////////////////////\n",
    "data = pd.DataFrame(columns=['input', pre_column])  # 建立新的数据矩阵\n",
    "parastr = ['volume', 'CODi', 'SSi', 'NH3-Ni', 'TPi', 'TNi', 'CODe', 'SSe', 'NH3Ne', 'TPe','TNe','T']\n",
    "parastr.remove(pre_column)\n",
    "print(parastr)\n",
    "\n",
    "\n",
    "for i in range(datanum):\n",
    "    Inputlist = []\n",
    "    for j in range(lookback):\n",
    "        inputlist = []\n",
    "        for k in range(paranum):\n",
    "            inputlist.append(rawdata_norm.loc[i+j][parastr[k]])\n",
    "        Inputlist.append(inputlist)\n",
    "    data.loc[i] = [Inputlist, result[i+lookback]]\n",
    "\n",
    "\n",
    "\n",
    "# 分割训练数据和测试数据\n",
    "fraction = 0.8  # 定义训练集、测试集切割比例//////////////////////////////////////////////////////////////////////////\n",
    "train_data = data.sample(frac=fraction)  # frac=0.8 means train data possess 80% of all\n",
    "print(len(train_data))\n",
    "test_data = data.drop(train_data.index)\n",
    "print(len(test_data))\n",
    "# 生成训练数据和测试数据，训练数据是240*5*7的数组\n",
    "trainnum = int(datanum * fraction)\n",
    "testnum = datanum - trainnum\n",
    "traindata = np.zeros((trainnum, lookback, paranum))\n",
    "testdata = np.zeros((testnum, lookback, paranum))\n",
    "\n",
    "for i in range(trainnum):\n",
    "    for j in range(lookback):\n",
    "        for k in range(paranum):\n",
    "            traindata[i][j][k] = train_data.iloc[i, 0][j][k]\n",
    "for i in range(testnum):\n",
    "    for j in range(lookback):\n",
    "        for k in range(paranum):\n",
    "            testdata[i][j][k] = test_data.iloc[i, 0][j][k]\n",
    "\n",
    "\n",
    "# 搭建神经网络\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "from keras import regularizers\n",
    "\n",
    "'''无dropout双层GRU'''\n",
    "model.add(layers.GRU(32,\n",
    "                     activation='sigmoid',\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(lookback, paranum)))\n",
    "model.add(layers.GRU(32, activation='sigmoid'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(0.01), loss='mae')\n",
    "print(model.summary())\n",
    "history = model.fit(traindata, train_data[pre_column],\n",
    "                    epochs=400,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "print(model.summary())\n",
    "\n",
    "# 可视化训练过程\n",
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend()\n",
    "display(plt.show())\n",
    "\n",
    "\n",
    "#  预测出水////////////////////////////////////////////////////////////////////////////////////////////\n",
    "predictdata = testdata\n",
    "predictnum = testnum\n",
    "predict = model.predict(predictdata).flatten()\n",
    "print(predict)\n",
    "\n",
    "\n",
    "# 结果可视化/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "plt.figure(figsize=(6, 6))\n",
    "true_value = test_data[pre_column]\n",
    "plt.scatter(predict, true_value)\n",
    "plt.ylabel('True Values')\n",
    "plt.xlabel('Predictions')\n",
    "plt.legend()\n",
    "display(plt.show())\n",
    "\n",
    "# 时序折线图\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(0, predictnum, 1)\n",
    "plt.plot(x,\n",
    "         predict,\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color='#ff9999',\n",
    "         marker=None,\n",
    "         markersize=6,\n",
    "         markeredgecolor='black',\n",
    "         markerfacecolor='#ff9999',\n",
    "         label='predict')\n",
    "plt.plot(x,\n",
    "         true_value,\n",
    "         linestyle='-',\n",
    "         linewidth=2,\n",
    "         color='steelblue',\n",
    "         marker=None,\n",
    "         markersize=6,\n",
    "         markeredgecolor='black',\n",
    "         markerfacecolor='steelblue',\n",
    "         label='actual')\n",
    "\n",
    "# 添加标题和坐标轴标签\n",
    "plt.title('2019predict')\n",
    "plt.xlabel('day')\n",
    "plt.ylabel(pre_column+'effluent')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 剔除图框上边界和右边界的刻度\n",
    "plt.tick_params(top='off', right='off')\n",
    "display(plt.show())\n",
    "\n",
    "\n",
    "# 计算mre/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "mre = 0\n",
    "for i in range(len(true_value)):\n",
    "    mre = mre + abs(true_value.iloc[i] - predict[i]) / true_value.iloc[i]\n",
    "    # 这里因为true_value的列表和real_predict的列表格式不同\n",
    "    #  所以计算时，使用了不同的索引级别\n",
    "Mre = mre / len(true_value)\n",
    "print('当前网络结构预测的'+pre_column+'出水MRE:' + str(Mre))\n",
    "print('训练集的真值平均值为：' + str(np.mean(true_value)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
